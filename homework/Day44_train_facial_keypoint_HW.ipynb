{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業\n",
    "請嘗試使用 flip (左右翻轉) 來做 augmentation 以降低人臉關鍵點檢測的 loss\n",
    "\n",
    "Note: 圖像 flip 之後，groundtruth 的關鍵點也要跟著 flip 哦\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例\n",
    "接下來的程式碼會示範如何定義一個簡單的 CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 colab 環境的同學請執行以下程式碼\n",
    "# %tensorflow_version 1.x # 確保 colob 中使用的 tensorflow 是 1.x 版本而不是 tensorflow 2\n",
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "\n",
    "# import os\n",
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive') # 將 google drive 掛載在 colob，\n",
    "# %cd 'gdrive/My Drive'\n",
    "# os.system(\"mkdir cupoy_cv_part4\") # 可以自己改路徑\n",
    "# %cd cupoy_cv_part4 # 可以自己改路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料集以及做前處理的函數\n",
    "def load_data(dirname):\n",
    "    # 讀取 csv 文件\n",
    "    data = pd.read_csv(dirname)\n",
    "    # 過濾有缺失值的 row\n",
    "    data = data.dropna()\n",
    "\n",
    "    # 將圖片像素值讀取為 numpy array 的形態\n",
    "    data['Image'] = data['Image'].apply(lambda img: np.fromstring(img, sep=' ')).values \n",
    "\n",
    "    # 單獨把圖像 array 抽取出來\n",
    "    imgs = np.vstack(data['Image'].values)/255\n",
    "    # reshape 為 96 x 96\n",
    "    imgs = imgs.reshape(data.shape[0], 96, 96)\n",
    "    # 轉換為 float\n",
    "    imgs = imgs.astype(np.float32)\n",
    "    \n",
    "    # 提取坐標的部分\n",
    "    points = data[data.columns[:-1]].values\n",
    "\n",
    "    # 轉換為 float\n",
    "    points = points.astype(np.float32)\n",
    "\n",
    "    # normalize 坐標值到 [-0.5, 0.5]\n",
    "    points = points/96 - 0.5\n",
    "    \n",
    "    return imgs, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圖像資料: (2140, 96, 96) \n",
      "關鍵點資料: (2140, 30)\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料\n",
    "imgs_train, points_train = load_data(dirname = 'training.csv')\n",
    "print(\"圖像資料:\", imgs_train.shape, \"\\n關鍵點資料:\", points_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回傳定義好的 model 的函數\n",
    "def get_model():\n",
    "    # 定義人臉關鍵點檢測網路\n",
    "    model = Sequential()\n",
    "\n",
    "    # 定義神經網路的輸入\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(96, 96, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # 最後輸出 30 維的向量，也就是 15 個關鍵點的值\n",
    "    model.add(Dense(30))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# 配置 loss funtion 和 optimizer\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 94, 94, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 47, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 45, 45, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 1,424,286\n",
      "Trainable params: 1,424,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 印出網路結構\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, History\n",
    "# model checkpoint \n",
    "checkpoint = ModelCheckpoint('best_weights.h5', verbose=1, save_best_only=True)\n",
    "hist = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1712 samples, validate on 428 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1712/1712 [==============================] - 9s 5ms/step - loss: 0.0076 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00236, saving model to best_weights.h5\n",
      "Epoch 2/150\n",
      "1712/1712 [==============================] - 0s 172us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00236 to 0.00199, saving model to best_weights.h5\n",
      "Epoch 3/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00199 to 0.00192, saving model to best_weights.h5\n",
      "Epoch 4/150\n",
      "1712/1712 [==============================] - 0s 176us/step - loss: 0.0012 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00192 to 0.00188, saving model to best_weights.h5\n",
      "Epoch 5/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 0.0011 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00188 to 0.00188, saving model to best_weights.h5\n",
      "Epoch 6/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 0.0010 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00188 to 0.00179, saving model to best_weights.h5\n",
      "Epoch 7/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 9.1328e-04 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00179 to 0.00163, saving model to best_weights.h5\n",
      "Epoch 8/150\n",
      "1712/1712 [==============================] - 0s 171us/step - loss: 7.8453e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00163 to 0.00154, saving model to best_weights.h5\n",
      "Epoch 9/150\n",
      "1712/1712 [==============================] - 0s 173us/step - loss: 7.1757e-04 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00154 to 0.00143, saving model to best_weights.h5\n",
      "Epoch 10/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 6.4350e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00143 to 0.00130, saving model to best_weights.h5\n",
      "Epoch 11/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 5.6327e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00130 to 0.00118, saving model to best_weights.h5\n",
      "Epoch 12/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 5.2621e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00118 to 0.00109, saving model to best_weights.h5\n",
      "Epoch 13/150\n",
      "1712/1712 [==============================] - 0s 171us/step - loss: 4.8532e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00109 to 0.00104, saving model to best_weights.h5\n",
      "Epoch 14/150\n",
      "1712/1712 [==============================] - 0s 171us/step - loss: 4.6078e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00104 to 0.00102, saving model to best_weights.h5\n",
      "Epoch 15/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 4.3434e-04 - val_loss: 9.9992e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00102 to 0.00100, saving model to best_weights.h5\n",
      "Epoch 16/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 4.1667e-04 - val_loss: 9.8925e-04\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00100 to 0.00099, saving model to best_weights.h5\n",
      "Epoch 17/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 3.9407e-04 - val_loss: 9.3047e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00099 to 0.00093, saving model to best_weights.h5\n",
      "Epoch 18/150\n",
      "1712/1712 [==============================] - 0s 168us/step - loss: 3.7111e-04 - val_loss: 9.0154e-04\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00093 to 0.00090, saving model to best_weights.h5\n",
      "Epoch 19/150\n",
      "1712/1712 [==============================] - 0s 169us/step - loss: 3.7182e-04 - val_loss: 8.9878e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00090 to 0.00090, saving model to best_weights.h5\n",
      "Epoch 20/150\n",
      "1712/1712 [==============================] - 0s 169us/step - loss: 3.5230e-04 - val_loss: 8.9357e-04\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00090 to 0.00089, saving model to best_weights.h5\n",
      "Epoch 21/150\n",
      "1712/1712 [==============================] - 0s 169us/step - loss: 3.3626e-04 - val_loss: 8.5839e-04\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00089 to 0.00086, saving model to best_weights.h5\n",
      "Epoch 22/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 3.2233e-04 - val_loss: 8.8081e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00086\n",
      "Epoch 23/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 3.0919e-04 - val_loss: 8.7418e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00086\n",
      "Epoch 24/150\n",
      "1712/1712 [==============================] - 0s 157us/step - loss: 3.1209e-04 - val_loss: 8.8643e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00086\n",
      "Epoch 25/150\n",
      "1712/1712 [==============================] - 0s 156us/step - loss: 3.0482e-04 - val_loss: 8.5744e-04\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00086 to 0.00086, saving model to best_weights.h5\n",
      "Epoch 26/150\n",
      "1712/1712 [==============================] - 0s 178us/step - loss: 2.9994e-04 - val_loss: 8.5138e-04\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00086 to 0.00085, saving model to best_weights.h5\n",
      "Epoch 27/150\n",
      "1712/1712 [==============================] - 0s 173us/step - loss: 2.8989e-04 - val_loss: 8.7107e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00085\n",
      "Epoch 28/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 2.8828e-04 - val_loss: 8.4800e-04\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00085 to 0.00085, saving model to best_weights.h5\n",
      "Epoch 29/150\n",
      "1712/1712 [==============================] - 0s 176us/step - loss: 2.7703e-04 - val_loss: 8.5260e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00085\n",
      "Epoch 30/150\n",
      "1712/1712 [==============================] - 0s 167us/step - loss: 2.6484e-04 - val_loss: 8.2226e-04\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00085 to 0.00082, saving model to best_weights.h5\n",
      "Epoch 31/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 2.5716e-04 - val_loss: 8.4699e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00082\n",
      "Epoch 32/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 2.5519e-04 - val_loss: 8.4021e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00082\n",
      "Epoch 33/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 2.4678e-04 - val_loss: 8.2511e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00082\n",
      "Epoch 34/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 2.4478e-04 - val_loss: 8.1307e-04\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00082 to 0.00081, saving model to best_weights.h5\n",
      "Epoch 35/150\n",
      "1712/1712 [==============================] - 0s 173us/step - loss: 2.3581e-04 - val_loss: 8.1415e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00081\n",
      "Epoch 36/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712/1712 [==============================] - 0s 162us/step - loss: 2.3586e-04 - val_loss: 8.8139e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00081\n",
      "Epoch 37/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 2.4056e-04 - val_loss: 8.4031e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00081\n",
      "Epoch 38/150\n",
      "1712/1712 [==============================] - 0s 170us/step - loss: 2.2381e-04 - val_loss: 8.0723e-04\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00081 to 0.00081, saving model to best_weights.h5\n",
      "Epoch 39/150\n",
      "1712/1712 [==============================] - 0s 170us/step - loss: 2.2792e-04 - val_loss: 8.4206e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00081\n",
      "Epoch 40/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 2.1794e-04 - val_loss: 7.8425e-04\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00081 to 0.00078, saving model to best_weights.h5\n",
      "Epoch 41/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 2.2345e-04 - val_loss: 7.7792e-04\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00078 to 0.00078, saving model to best_weights.h5\n",
      "Epoch 42/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 2.1598e-04 - val_loss: 8.2867e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00078\n",
      "Epoch 43/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 2.0763e-04 - val_loss: 8.0091e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00078\n",
      "Epoch 44/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 2.0901e-04 - val_loss: 7.9301e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00078\n",
      "Epoch 45/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 2.0905e-04 - val_loss: 8.1607e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00078\n",
      "Epoch 46/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 2.0136e-04 - val_loss: 7.8946e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00078\n",
      "Epoch 47/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.9910e-04 - val_loss: 7.7152e-04\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00078 to 0.00077, saving model to best_weights.h5\n",
      "Epoch 48/150\n",
      "1712/1712 [==============================] - 0s 172us/step - loss: 1.9286e-04 - val_loss: 7.7569e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00077\n",
      "Epoch 49/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 2.0182e-04 - val_loss: 7.6645e-04\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00077 to 0.00077, saving model to best_weights.h5\n",
      "Epoch 50/150\n",
      "1712/1712 [==============================] - 0s 172us/step - loss: 1.9179e-04 - val_loss: 7.8643e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00077\n",
      "Epoch 51/150\n",
      "1712/1712 [==============================] - 0s 167us/step - loss: 1.8723e-04 - val_loss: 7.7939e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00077\n",
      "Epoch 52/150\n",
      "1712/1712 [==============================] - 0s 178us/step - loss: 1.8155e-04 - val_loss: 7.9438e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00077\n",
      "Epoch 53/150\n",
      "1712/1712 [==============================] - 0s 176us/step - loss: 1.8310e-04 - val_loss: 8.4957e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00077\n",
      "Epoch 54/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.8290e-04 - val_loss: 8.1775e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00077\n",
      "Epoch 55/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.8171e-04 - val_loss: 7.5816e-04\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00077 to 0.00076, saving model to best_weights.h5\n",
      "Epoch 56/150\n",
      "1712/1712 [==============================] - 0s 171us/step - loss: 1.7592e-04 - val_loss: 7.7786e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00076\n",
      "Epoch 57/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.7427e-04 - val_loss: 7.7420e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00076\n",
      "Epoch 58/150\n",
      "1712/1712 [==============================] - 0s 156us/step - loss: 1.7449e-04 - val_loss: 7.8331e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00076\n",
      "Epoch 59/150\n",
      "1712/1712 [==============================] - 0s 177us/step - loss: 1.7061e-04 - val_loss: 7.9081e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00076\n",
      "Epoch 60/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 1.7016e-04 - val_loss: 7.7016e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00076\n",
      "Epoch 61/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.6947e-04 - val_loss: 7.7040e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00076\n",
      "Epoch 62/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 1.7108e-04 - val_loss: 7.9261e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00076\n",
      "Epoch 63/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.6766e-04 - val_loss: 7.7481e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00076\n",
      "Epoch 64/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.6277e-04 - val_loss: 7.8279e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00076\n",
      "Epoch 65/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.5779e-04 - val_loss: 7.6502e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00076\n",
      "Epoch 66/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.5642e-04 - val_loss: 7.9644e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00076\n",
      "Epoch 67/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 1.5344e-04 - val_loss: 7.7345e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00076\n",
      "Epoch 68/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.5428e-04 - val_loss: 7.8986e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00076\n",
      "Epoch 69/150\n",
      "1712/1712 [==============================] - 0s 157us/step - loss: 1.5553e-04 - val_loss: 7.7603e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00076\n",
      "Epoch 70/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.5054e-04 - val_loss: 7.7326e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00076\n",
      "Epoch 71/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 1.4909e-04 - val_loss: 7.6780e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00076\n",
      "Epoch 72/150\n",
      "1712/1712 [==============================] - 0s 167us/step - loss: 1.5458e-04 - val_loss: 8.1850e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00076\n",
      "Epoch 73/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.5649e-04 - val_loss: 7.8197e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00076\n",
      "Epoch 74/150\n",
      "1712/1712 [==============================] - 0s 157us/step - loss: 1.4593e-04 - val_loss: 8.0260e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00076\n",
      "Epoch 75/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.4635e-04 - val_loss: 8.2528e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00076\n",
      "Epoch 76/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 1.4744e-04 - val_loss: 8.0140e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00076\n",
      "Epoch 77/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 1.4884e-04 - val_loss: 8.0357e-04\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00076\n",
      "Epoch 78/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.4512e-04 - val_loss: 7.5473e-04\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00076 to 0.00075, saving model to best_weights.h5\n",
      "Epoch 79/150\n",
      "1712/1712 [==============================] - 0s 179us/step - loss: 1.4434e-04 - val_loss: 7.9756e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00075\n",
      "Epoch 80/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.4152e-04 - val_loss: 7.9864e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00075\n",
      "Epoch 81/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.3666e-04 - val_loss: 8.0118e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00075\n",
      "Epoch 82/150\n",
      "1712/1712 [==============================] - 0s 169us/step - loss: 1.3393e-04 - val_loss: 7.7072e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00075\n",
      "Epoch 83/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.3268e-04 - val_loss: 7.6248e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00075\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.3179e-04 - val_loss: 7.9779e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00075\n",
      "Epoch 85/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.3749e-04 - val_loss: 8.4200e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00075\n",
      "Epoch 86/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 1.3457e-04 - val_loss: 7.6213e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00075\n",
      "Epoch 87/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 1.3014e-04 - val_loss: 7.7103e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00075\n",
      "Epoch 88/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.3192e-04 - val_loss: 7.8778e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00075\n",
      "Epoch 89/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.2967e-04 - val_loss: 7.8785e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00075\n",
      "Epoch 90/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.2788e-04 - val_loss: 7.6694e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00075\n",
      "Epoch 91/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.3094e-04 - val_loss: 7.6693e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00075\n",
      "Epoch 92/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 1.2235e-04 - val_loss: 7.9924e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00075\n",
      "Epoch 93/150\n",
      "1712/1712 [==============================] - 0s 154us/step - loss: 1.2507e-04 - val_loss: 8.1430e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00075\n",
      "Epoch 94/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.2634e-04 - val_loss: 8.1138e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00075\n",
      "Epoch 95/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.2820e-04 - val_loss: 7.9395e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00075\n",
      "Epoch 96/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 1.2701e-04 - val_loss: 7.9118e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00075\n",
      "Epoch 97/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.2626e-04 - val_loss: 8.0774e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00075\n",
      "Epoch 98/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.2452e-04 - val_loss: 7.9412e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00075\n",
      "Epoch 99/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 1.1835e-04 - val_loss: 7.7680e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00075\n",
      "Epoch 100/150\n",
      "1712/1712 [==============================] - 0s 157us/step - loss: 1.1968e-04 - val_loss: 7.7354e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00075\n",
      "Epoch 101/150\n",
      "1712/1712 [==============================] - 0s 169us/step - loss: 1.1820e-04 - val_loss: 7.7088e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00075\n",
      "Epoch 102/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.1950e-04 - val_loss: 7.9610e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00075\n",
      "Epoch 103/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.1825e-04 - val_loss: 7.9311e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00075\n",
      "Epoch 104/150\n",
      "1712/1712 [==============================] - 0s 170us/step - loss: 1.1600e-04 - val_loss: 7.9046e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00075\n",
      "Epoch 105/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.1331e-04 - val_loss: 7.7867e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00075\n",
      "Epoch 106/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.0984e-04 - val_loss: 7.7280e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00075\n",
      "Epoch 107/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.1372e-04 - val_loss: 7.8519e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00075\n",
      "Epoch 108/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.1355e-04 - val_loss: 8.1967e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00075\n",
      "Epoch 109/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.1217e-04 - val_loss: 7.6680e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00075\n",
      "Epoch 110/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.1272e-04 - val_loss: 8.1415e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00075\n",
      "Epoch 111/150\n",
      "1712/1712 [==============================] - 0s 162us/step - loss: 1.1290e-04 - val_loss: 7.6997e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00075\n",
      "Epoch 112/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.1056e-04 - val_loss: 7.7542e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00075\n",
      "Epoch 113/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 1.0697e-04 - val_loss: 7.8869e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00075\n",
      "Epoch 114/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.0887e-04 - val_loss: 7.7389e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00075\n",
      "Epoch 115/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.0962e-04 - val_loss: 7.9102e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00075\n",
      "Epoch 116/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.0855e-04 - val_loss: 7.8952e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00075\n",
      "Epoch 117/150\n",
      "1712/1712 [==============================] - 0s 164us/step - loss: 1.1014e-04 - val_loss: 7.9456e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00075\n",
      "Epoch 118/150\n",
      "1712/1712 [==============================] - 0s 166us/step - loss: 1.0890e-04 - val_loss: 7.5449e-04\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00075 to 0.00075, saving model to best_weights.h5\n",
      "Epoch 119/150\n",
      "1712/1712 [==============================] - 0s 165us/step - loss: 1.0902e-04 - val_loss: 8.0142e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00075\n",
      "Epoch 120/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.0501e-04 - val_loss: 8.3334e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00075\n",
      "Epoch 121/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.0586e-04 - val_loss: 8.0186e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00075\n",
      "Epoch 122/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.0360e-04 - val_loss: 7.8460e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00075\n",
      "Epoch 123/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 1.0166e-04 - val_loss: 7.9830e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00075\n",
      "Epoch 124/150\n",
      "1712/1712 [==============================] - 0s 154us/step - loss: 1.0387e-04 - val_loss: 7.8189e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00075\n",
      "Epoch 125/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 1.0384e-04 - val_loss: 8.0196e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00075\n",
      "Epoch 126/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 1.0163e-04 - val_loss: 8.0164e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00075\n",
      "Epoch 127/150\n",
      "1712/1712 [==============================] - 0s 169us/step - loss: 1.0184e-04 - val_loss: 8.1143e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00075\n",
      "Epoch 128/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 1.0089e-04 - val_loss: 7.8934e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00075\n",
      "Epoch 129/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 9.7751e-05 - val_loss: 8.0586e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00075\n",
      "Epoch 130/150\n",
      "1712/1712 [==============================] - 0s 171us/step - loss: 9.7937e-05 - val_loss: 7.9815e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00075\n",
      "Epoch 131/150\n",
      "1712/1712 [==============================] - 0s 163us/step - loss: 9.9404e-05 - val_loss: 8.0881e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00075\n",
      "Epoch 132/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 9.6397e-05 - val_loss: 7.8088e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00075\n",
      "Epoch 133/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712/1712 [==============================] - 0s 156us/step - loss: 9.5860e-05 - val_loss: 7.9897e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00075\n",
      "Epoch 134/150\n",
      "1712/1712 [==============================] - 0s 159us/step - loss: 9.6410e-05 - val_loss: 7.7541e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00075\n",
      "Epoch 135/150\n",
      "1712/1712 [==============================] - 0s 156us/step - loss: 9.7864e-05 - val_loss: 7.9035e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00075\n",
      "Epoch 136/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 9.8500e-05 - val_loss: 7.9526e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00075\n",
      "Epoch 137/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 9.7722e-05 - val_loss: 8.0246e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00075\n",
      "Epoch 138/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 9.5387e-05 - val_loss: 7.6704e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00075\n",
      "Epoch 139/150\n",
      "1712/1712 [==============================] - 0s 156us/step - loss: 9.4909e-05 - val_loss: 7.8174e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00075\n",
      "Epoch 140/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 9.4899e-05 - val_loss: 8.0025e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00075\n",
      "Epoch 141/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 9.5086e-05 - val_loss: 8.1010e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00075\n",
      "Epoch 142/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 9.2893e-05 - val_loss: 8.0406e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00075\n",
      "Epoch 143/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 9.1022e-05 - val_loss: 7.8407e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00075\n",
      "Epoch 144/150\n",
      "1712/1712 [==============================] - 0s 155us/step - loss: 9.0654e-05 - val_loss: 7.9677e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00075\n",
      "Epoch 145/150\n",
      "1712/1712 [==============================] - 0s 161us/step - loss: 9.2272e-05 - val_loss: 8.1487e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00075\n",
      "Epoch 146/150\n",
      "1712/1712 [==============================] - 0s 158us/step - loss: 9.2957e-05 - val_loss: 8.0218e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00075\n",
      "Epoch 147/150\n",
      "1712/1712 [==============================] - 0s 154us/step - loss: 9.2803e-05 - val_loss: 7.8477e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00075\n",
      "Epoch 148/150\n",
      "1712/1712 [==============================] - 0s 157us/step - loss: 8.9799e-05 - val_loss: 8.0982e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00075\n",
      "Epoch 149/150\n",
      "1712/1712 [==============================] - 0s 160us/step - loss: 8.9366e-05 - val_loss: 7.8365e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00075\n",
      "Epoch 150/150\n",
      "1712/1712 [==============================] - 0s 156us/step - loss: 9.1873e-05 - val_loss: 7.8080e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00075\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "hist_model = model.fit(imgs_train.reshape(-1, 96, 96, 1), \n",
    "                       points_train, \n",
    "                       validation_split=0.2, batch_size=64, callbacks=[checkpoint, hist],\n",
    "                       shuffle=True, epochs=150, verbose=1)\n",
    "# save the model weights\n",
    "model.save_weights('weights.h5')\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb1156c5f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loss 值的圖\n",
    "plt.title('Optimizer : Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(hist_model.history['loss'], color='b', label='Training Loss')\n",
    "plt.plot(hist_model.history['val_loss'], color='r', label='Validation Loss')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 觀察 model 在 testing 上的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取測試資料集\n",
    "imgs_test, _ = load_data(dirname = 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在灰階圖像上畫關鍵點的函數\n",
    "def plot_keypoints(img, points):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    for i in range(0,30,2):\n",
    "        plt.scatter((points[i] + 0.5)*96, (points[i+1]+0.5)*96, color='red')"
   ]
  },
  {
   "c